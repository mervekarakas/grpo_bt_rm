# -------- dataset + output --------
export DATASET="${DATASET:-/data/$USER/datasets/grpo_bt_hh_train_hh_score100_v3.jsonl}"
export OUT_DIR="${OUT_DIR:-/data/$USER/experiments/grpo_bt_rm/hh_v3_T5_NC2_offset_base}"

# -------- model (base, no SFT) --------
export MODEL="${MODEL:-Qwen/Qwen2.5-7B-Instruct}"

# -------- swift plugin wiring --------
export PLUGIN="${PLUGIN:-src/grpo_bt_rm/training/reward_plugins/bt_baseline.py}"
export REWARD_NAME="${REWARD_NAME:-bt_pointwise_baseline}"

# -------- reward parsing + scaling --------
export BT_SCORE_PARSER="${BT_SCORE_PARSER:-score100_last}"
export BT_DELTA_TEMP="${BT_DELTA_TEMP:-5}"
export BT_DELTA_CLIP="${BT_DELTA_CLIP:-0}"           # no positive clip
export BT_DELTA_NEG_CLIP="${BT_DELTA_NEG_CLIP:-2}"   # cap worst-case at log Ïƒ(-2)
export BT_REWARD_SCALE="${BT_REWARD_SCALE:-1}"
export BT_REWARD_OFFSET="${BT_REWARD_OFFSET:-0.693}"  # shift so tie=0

# -------- generation --------
export TEMPERATURE="${TEMPERATURE:-1.0}"              # best from sweep for v3+chat
export NUM_GENERATIONS="${NUM_GENERATIONS:-8}"
export MAX_COMPLETION_LENGTH="${MAX_COMPLETION_LENGTH:-128}"
export MAX_NEW_TOKENS="${MAX_NEW_TOKENS:-128}"

# -------- training --------
export LEARNING_RATE="${LEARNING_RATE:-5e-6}"

# -------- nice-to-have: default eval prompt/parser for tools --------
export EVAL_DATASET="${EVAL_DATASET:-anthropic_hh}"
export EVAL_PROMPT="${EVAL_PROMPT:-hh_score100_v3}"
export EVAL_PARSER="${EVAL_PARSER:-score100_last}"
export EVAL_BT_TEMP="${EVAL_BT_TEMP:-5}"
export EVAL_UNC_LOW="${EVAL_UNC_LOW:-20}"
export EVAL_UNC_HIGH="${EVAL_UNC_HIGH:-80}"
