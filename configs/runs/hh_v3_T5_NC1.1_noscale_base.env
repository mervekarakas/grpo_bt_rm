# -------- dataset + output --------
export DATASET="${DATASET:-/data/$USER/datasets/grpo_bt_hh_train_hh_score100_v3.jsonl}"
export OUT_DIR="${OUT_DIR:-/data/$USER/experiments/grpo_bt_rm/hh_v3_T5_NC1.1_noscale_base}"

# -------- model (base, no SFT) --------
export MODEL="${MODEL:-Qwen/Qwen2.5-7B-Instruct}"

# -------- swift plugin wiring --------
export PLUGIN="${PLUGIN:-src/grpo_bt_rm/training/reward_plugins/bt_baseline.py}"
export REWARD_NAME="${REWARD_NAME:-bt_pointwise_baseline}"

# -------- reward parsing + scaling --------
export BT_SCORE_PARSER="${BT_SCORE_PARSER:-score100_last}"
export BT_DELTA_TEMP="${BT_DELTA_TEMP:-5}"
export BT_DELTA_CLIP="${BT_DELTA_CLIP:-0}"           # no positive clip
export BT_DELTA_NEG_CLIP="${BT_DELTA_NEG_CLIP:-1.1}" # symmetric clip
export BT_REWARD_SCALE="${BT_REWARD_SCALE:-14.4}"    # map to ~[-10, +10]
export BT_REWARD_OFFSET="${BT_REWARD_OFFSET:-10.0}"  # shift so tieâ‰ˆ0

# -------- generation --------
export TEMPERATURE="${TEMPERATURE:-1.0}"
export NUM_GENERATIONS="${NUM_GENERATIONS:-8}"
export MAX_COMPLETION_LENGTH="${MAX_COMPLETION_LENGTH:-128}"
export MAX_NEW_TOKENS="${MAX_NEW_TOKENS:-128}"

# -------- training --------
export LEARNING_RATE="${LEARNING_RATE:-5e-6}"

# -------- disable std normalization so reward scale matters --------
export EXTRA_ARGS="${EXTRA_ARGS:---scale_rewards none}"

# -------- nice-to-have: default eval prompt/parser for tools --------
export EVAL_DATASET="${EVAL_DATASET:-anthropic_hh}"
export EVAL_PROMPT="${EVAL_PROMPT:-hh_score100_v3}"
export EVAL_PARSER="${EVAL_PARSER:-score100_last}"
export EVAL_BT_TEMP="${EVAL_BT_TEMP:-5}"
export EVAL_UNC_LOW="${EVAL_UNC_LOW:-20}"
export EVAL_UNC_HIGH="${EVAL_UNC_HIGH:-80}"
